{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5fa88b",
   "metadata": {},
   "source": [
    "### Report on Machine Learning Project\n",
    "# FAKE NEWS CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96bc63",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fe157",
   "metadata": {},
   "source": [
    "- Fake news classification refers to the process of identifying and categorizing news articles or other types of media that contain intentionally misleading, false, or fabricated information. With the increasing prevalence of social media and online news sources, fake news has become a serious problem that can have far-reaching consequences. It can be used to spread misinformation, influence public opinion, and even manipulate political outcomes.\n",
    "\n",
    "- To address this problem, researchers and experts in the field have developed a number of different techniques and tools for detecting and classifying fake news. Some of the most commonly used methods include machine learning algorithms, natural language processing, and network analysis. These techniques allow analysts to analyze the content and structure of news articles, identify patterns and features that distinguish fake from real news, and classify articles into different categories based on their level of credibility.\n",
    "\n",
    "- However, detecting and classifying fake news is not always straightforward. One of the key challenges is the dynamic and evolving nature of the phenomenon. Fake news can take many different forms, and new techniques for spreading false information are constantly being developed. As a result, fake news classification requires ongoing research and development of new methods and approaches.\n",
    "\n",
    "- Despite these challenges, the importance of fake news classification cannot be overstated. Fake news can have serious consequences in various areas, such as politics, public health, and security. It can spread misinformation, undermine trust in the media and other information sources, propagate biased or discriminatory views, and even be used to manipulate public opinion or behavior. By detecting and labeling fake news, we can reduce the impact of these negative effects and help people make more informed decisions based on accurate and trustworthy information.\n",
    "\n",
    "- In addition, fake news classification can help promote a more informed and democratic society. When people have access to reliable information, they are better equipped to participate in public discourse, make informed decisions, and hold their leaders accountable. This can contribute to a more just and equitable society, where individuals are empowered to advocate for their own interests and those of their communities.\n",
    "\n",
    "- Starting with making the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe1886",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0d023",
   "metadata": {},
   "source": [
    "## Reading the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de930a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-Processing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "# Data  Visualisation Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go \n",
    "# Handling Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Text Pre-Processing Libraries\n",
    "import re\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Fake.csv')\n",
    "real =pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd1efd",
   "metadata": {},
   "source": [
    "## Cleaning and Pre-processing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7e601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f267ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c93983",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fea377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1314fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ceade",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917da468",
   "metadata": {},
   "outputs": [],
   "source": [
    "real['category'] = 0\n",
    "fake['category'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b6d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news = pd.concat([real,fake],axis=0,ignore_index=True)\n",
    "# previewing the new dataset\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba018423",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns not to be used\n",
    "news.drop(['title','subject','date'],axis=1,inplace=True)\n",
    "# Removing all punctuations\n",
    "import re\n",
    "news['text'] = news['text'].map(lambda x: re.sub('[-,\\.!?]', '', x))\n",
    "# Converting the text data to lower case\n",
    "news['text'] = news['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac612432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the different processed titles together.\n",
    "long_string = ' '.join(news['text'])\n",
    "\n",
    "# Creating a WordCloud object\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "# Generating a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualizing the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280867cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the English language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Parsing the text with Spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatizing the tokens and remove stop words\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    # Joining the lemmas back into a string and return it\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# applying the preprocess_text function to the text column\n",
    "news['text'] = news['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a28098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading splitting library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Defining the independent variable\n",
    "X = news['text']\n",
    "\n",
    "# Defining the dependent variable\n",
    "y = news['category']\n",
    "\n",
    "# Splitting the data into training and testing set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading count vectorizer library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiating count vectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fitting and transforming X train \n",
    "X_train_vect = cv.fit_transform(X_train)\n",
    "\n",
    "# Tranforming X test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa5679",
   "metadata": {},
   "source": [
    "## Modeling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c228caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating logistic regression\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(X_train_vect,y_train)\n",
    "\n",
    "# Predicting the value of y_train using the model\n",
    "y_pred_train = logreg.predict(X_train_vect)\n",
    "\n",
    "# Predicting the value of y_test using the model\n",
    "y_pred_test = logreg.predict(X_test_vect)\n",
    "\n",
    "\n",
    "# Accuracy of the training and testing data\n",
    "train_accuracy = accuracy_score(y_train,y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test,y_pred_test)\n",
    "print(f'Train accuracy - {train_accuracy} \\nTest accuracy - {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e11509",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(y_test, y_pred_test)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for training data\n",
    "categories=['real','fake']\n",
    "print(classification_report(y_train,y_pred_train,target_names=categories,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for testing data\n",
    "print(classification_report(y_test,y_pred_test,target_names=categories,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a9729",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "Overall, the field of fake news classification is constantly evolving, as researchers and experts work to develop new and more effective methods for detecting and classifying fake news. By continuing to invest in this field, we can help combat the spread of fake news and promote a more informed and democratic society that values accuracy, transparency, and accountability. After analyzing the model's performance, it can be concluded that it displays 99% accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0aa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
